{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNapcI9S8S7RIidl3xjy/ji",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manuelnapolesromero/-Levantar-un-Nodo-Hadoop-con-Docker/blob/main/Optimizaci%C3%B3n_del_procesamiento_de_datos_neurofisiol%C3%B3gicos_para_el_momo_RR34.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Written by Sebastian Pujalte for the Rossi Pool Lab 01/2022.\n",
        "Updated and improved for robustness and clarity.\n",
        "Script for processing all .nev data and associated .mat psychophysical data for the RR34 monkey.\n",
        "\n",
        "This script processes raw neurophysiological data (.nev files) and corresponding psychophysical\n",
        "data (.mat files) into a standardized, clean format suitable for downstream analysis,\n",
        "such as spike sorting with TopoSort.\n",
        "\n",
        "TODO:\n",
        "- Review and generalize the mat_processer for other monkeys if needed.\n",
        "- Consider adding more detailed error handling for file parsing issues (e.g., corrupted files).\n",
        "- Evaluate performance with very large datasets and potentially optimize I/O operations.\n",
        "\"\"\"\n",
        "\n",
        "from toposort.pipeline import spike_pipeline, trailing_number\n",
        "import logging\n",
        "import numpy as np\n",
        "from brpylib import NevFile, brpylib_ver\n",
        "from scipy.io import loadmat\n",
        "import os\n",
        "from os.path import join as pjoin\n",
        "from pandas import DataFrame\n",
        "from logging import info, error, warning\n",
        "\n",
        "# --- Version Control ---\n",
        "BR_PYLIB_VER_REQ = \"1.3.1\"\n",
        "# It's good practice to ensure brpylib is imported successfully before checking its version.\n",
        "# The original code already ensures this by being at the top level.\n",
        "if brpylib_ver.split(\".\") < BR_PYLIB_VER_REQ.split(\".\"):\n",
        "    raise ImportError(\n",
        "        f\"Requires brpylib {BR_PYLIB_VER_REQ} or higher. Please update your brpylib package.\"\n",
        "    )\n",
        "\n",
        "def nev_processer(filepath: str, savepath: str = None) -> dict:\n",
        "    \"\"\" Processes all .nev files within a given directory into a cleaner format.\n",
        "        Data is separated into individual electrodes, and spike trial/time\n",
        "        information is saved into .csv files. Waveforms are returned for\n",
        "        downstream processing (e.g., alignment).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filepath : str\n",
        "        Path to the directory where the .nev files are stored.\n",
        "    savepath : str, optional\n",
        "        Path to the directory where you wish to save processed .nev data (.csv files).\n",
        "        If None, data will be returned by the function but not saved to disk.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    data : dict\n",
        "        A dictionary where keys are electrode identifiers (e.g., 'e1', 'e2').\n",
        "        Each electrode's entry contains 'STime', 'Waveforms', and 'STrial' lists/arrays.\n",
        "    \"\"\"\n",
        "    if not os.path.isdir(filepath):\n",
        "        error(f\"Provided filepath is not a valid directory: {filepath}. Returning empty data.\")\n",
        "        return {}\n",
        "\n",
        "    # Filter for .nev files and sort them by trailing number (trial number)\n",
        "    all_nev_files = [\n",
        "        f for f in os.listdir(filepath) if f.endswith(\".nev\") and f.startswith(\"d\")\n",
        "    ]\n",
        "\n",
        "    # Handle the case where no files are found more gracefully for sorting\n",
        "    if not all_nev_files:\n",
        "        info(f\"No .nev files found in {filepath} matching 'd*.nev' pattern. Returning empty data.\")\n",
        "        return {}\n",
        "\n",
        "    nev_files_with_numbers = []\n",
        "    for f in all_nev_files:\n",
        "        num = trailing_number(f[:-4])\n",
        "        if num is not None:\n",
        "            nev_files_with_numbers.append((num, f))\n",
        "        else:\n",
        "            # Using warning here is appropriate if it's not a fatal error for the whole process\n",
        "            warning(f\"Could not extract trial number from {f}. Skipping file for processing.\")\n",
        "\n",
        "    # If all files were skipped due to missing trial numbers\n",
        "    if not nev_files_with_numbers:\n",
        "        info(f\"All .nev files in {filepath} were skipped due to missing trial numbers. Returning empty data.\")\n",
        "        return {}\n",
        "\n",
        "    nev_files_with_numbers.sort()\n",
        "    sorted_nev_files = [f for _, f in nev_files_with_numbers]\n",
        "\n",
        "    # min_trial calculation should only happen if nev_files_with_numbers is not empty\n",
        "    min_trial = nev_files_with_numbers[0][0]\n",
        "\n",
        "    data = {} # Initialize dictionary to hold processed data\n",
        "\n",
        "    for f in sorted_nev_files:\n",
        "        full_path = pjoin(filepath, f)\n",
        "        # We already ensured num is not None when adding to nev_files_with_numbers\n",
        "        trial = int(trailing_number(f[:-4]))\n",
        "\n",
        "        try:\n",
        "            with NevFile(full_path) as file:\n",
        "                # Request a reasonable range of channels (0-128 is common for many systems)\n",
        "                # This avoids magic numbers like 33 and is more general.\n",
        "                # Consider making this range configurable if needed for different setups.\n",
        "                nev_data = file.getdata(elec_ids=list(range(129)))\n",
        "\n",
        "                # Check if 'spike_events' key exists and contains data\n",
        "                if \"spike_events\" not in nev_data or not nev_data[\"spike_events\"]:\n",
        "                    info(f\"No spike events found in {f} or 'spike_events' is empty. Skipping file.\")\n",
        "                    continue\n",
        "\n",
        "                electrodes = nev_data[\"spike_events\"][\"ChannelID\"]\n",
        "                waveforms = nev_data[\"spike_events\"][\"Waveforms\"]\n",
        "                spike_times = nev_data[\"spike_events\"][\"TimeStamps\"]\n",
        "\n",
        "        except Exception as e:\n",
        "            # Catching a more specific exception like brpylib.NevFileError might be better\n",
        "            # if brpylib offers it, but a general Exception is fine for robustness here.\n",
        "            error(f\"Error processing .nev file {f}: {e}. Skipping to next file.\")\n",
        "            continue\n",
        "\n",
        "        for i, elect_num in enumerate(electrodes):\n",
        "            # It's safer to check if waveforms[i] is iterable and not empty.\n",
        "            # Some libraries might return None or an empty array/list.\n",
        "            if not isinstance(waveforms[i], (list, np.ndarray)) or not len(waveforms[i]):\n",
        "                warning(f\"Electrode {elect_num} in {f} has no waveforms or invalid data. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            n_spikes = len(waveforms[i])\n",
        "\n",
        "            # Initialize electrode entry if it doesn't exist\n",
        "            # Using dict.setdefault is a slightly more concise way to do this.\n",
        "            data.setdefault(f\"e{elect_num}\", {\"STime\": [], \"Waveforms\": [], \"STrial\": []})\n",
        "\n",
        "            # Use extend for lists for efficiency\n",
        "            data[f\"e{elect_num}\"][\"STime\"].extend(spike_times[i])\n",
        "            data[f\"e{elect_num}\"][\"Waveforms\"].extend(waveforms[i])\n",
        "            data[f\"e{elect_num}\"][\"STrial\"].extend(\n",
        "                np.full(n_spikes, trial, dtype=int).tolist() # Convert back to list if using extend\n",
        "            )\n",
        "\n",
        "    # --- Post-processing and Filtering ---\n",
        "    # Convert lists to NumPy arrays and filter electrodes with few spikes\n",
        "    electrodes_to_delete = []\n",
        "    for e_key in list(data.keys()): # Iterate over a copy of keys for safe deletion\n",
        "        total_n_spikes = len(data[e_key][\"STrial\"])\n",
        "        if total_n_spikes > 100:\n",
        "            # Convert lists to NumPy arrays after all data has been aggregated\n",
        "            # Ensure dtypes are appropriate for storage/downstream use.\n",
        "            # Using .astype() if source might be float or mixed, otherwise just np.array(..., dtype=...)\n",
        "            data[e_key][\"STime\"] = np.array(data[e_key][\"STime\"], dtype=np.int64)\n",
        "            data[e_key][\"Waveforms\"] = np.array(data[e_key][\"Waveforms\"], dtype=np.int16)\n",
        "            data[e_key][\"STrial\"] = np.array(data[e_key][\"STrial\"], dtype=np.int32)\n",
        "        else:\n",
        "            electrodes_to_delete.append(e_key)\n",
        "            info(\n",
        "                f\"Electrode {e_key} contains only {total_n_spikes} spike(s). \"\n",
        "                \"This electrode will be ignored (assumed error or insufficient data).\"\n",
        "            )\n",
        "\n",
        "    for e_key in electrodes_to_delete:\n",
        "        del data[e_key]\n",
        "\n",
        "    # Regularize trial numbers to start from 1\n",
        "    # This block only executes if min_trial was successfully found and is > 1.\n",
        "    if min_trial > 1:\n",
        "        info(f\"Normalizing trial numbers. Original minimum trial: {min_trial}. New minimum trial: 1.\")\n",
        "        for e_key in data.keys():\n",
        "            data[e_key][\"STrial\"] -= (min_trial - 1)\n",
        "\n",
        "    # --- Optional Saving to CSV ---\n",
        "    if savepath is not None:\n",
        "        try:\n",
        "            os.makedirs(savepath, exist_ok=True)\n",
        "            info(f\"Ensured save directory exists: {savepath}\")\n",
        "        except OSError as e: # Catch specific OS errors for directory creation\n",
        "            error(f\"Failed to create save directory {savepath}: {e}. Skipping CSV saving.\")\n",
        "            return data # Exit early if savepath can't be created\n",
        "\n",
        "        for e_key in data.keys():\n",
        "            electrode_save_path = pjoin(savepath, e_key)\n",
        "            try:\n",
        "                os.makedirs(electrode_save_path, exist_ok=True)\n",
        "            except OSError as e:\n",
        "                error(f\"Failed to create electrode directory {electrode_save_path}: {e}. Skipping CSV saving for this electrode.\")\n",
        "                continue # Skip to next electrode\n",
        "\n",
        "            # Save STime and STrial as CSV. Waveforms are handled downstream.\n",
        "            try:\n",
        "                np.savetxt(\n",
        "                    fname=pjoin(electrode_save_path, \"STime.csv\"),\n",
        "                    X=data[e_key][\"STime\"],\n",
        "                    fmt=\"%i\",\n",
        "                    delimiter=\",\"\n",
        "                )\n",
        "                np.savetxt(\n",
        "                    fname=pjoin(electrode_save_path, \"STrial.csv\"),\n",
        "                    X=data[e_key][\"STrial\"],\n",
        "                    fmt=\"%i\",\n",
        "                    delimiter=\",\"\n",
        "                )\n",
        "                info(f\"Saved STime.csv and STrial.csv for {e_key} in {electrode_save_path}\")\n",
        "            except Exception as e_save:\n",
        "                error(f\"Error saving CSVs for {e_key} at {electrode_save_path}: {e_save}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def mat_processer(filepath: str, savepath: str = None, monkey_prefix: str = \"RR034\") -> DataFrame:\n",
        "    \"\"\" Processes .mat files with psychophysical information about the experiment\n",
        "        into a friendlier .csv format. This function is currently tailored\n",
        "        for specific monkey data formats but can be adapted.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filepath : str\n",
        "        Path to the directory where the .mat file is stored. Note that this function\n",
        "        assumes there is typically one relevant .mat file per monkey/experiment.\n",
        "    savepath : str, optional\n",
        "        Path to directory where you wish to save processed psychophysical information.\n",
        "        If None, then the DataFrame will be returned but not saved to file.\n",
        "    monkey_prefix : str, optional\n",
        "        The file prefix used to identify the relevant .mat file (e.g., \"RR034\").\n",
        "        Defaults to \"RR034\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    psico_df : DataFrame\n",
        "        Returns a Pandas DataFrame where each row is a trial and columns\n",
        "        correspond to psychophysical information. Returns an empty DataFrame\n",
        "        if processing fails.\n",
        "    \"\"\"\n",
        "    COLS = [\n",
        "        \"Trial\", \"Class\", \"SelectedButton\", \"CorrectButton\", \"Hits\",\n",
        "        \"AmpStim1\", \"FreqStim1\", \"AudStim1\", \"AmpStim2\", \"FreqStim2\", \"AudStim2\",\n",
        "        \"AmpStim3\", \"FreqStim3\", \"AudStim3\", \"RelKD\", \"RelKU\", \"RelMov\", \"Luz\",\n",
        "        \"TRW\", \"Set\", \"pd\", \"kd\", \"o1\", \"f1\", \"o2\", \"f2\", \"o3\", \"f3\", \"pu\", \"ku\", \"pb\",\n",
        "    ]\n",
        "\n",
        "    if not os.path.isdir(filepath):\n",
        "        error(f\"Provided filepath is not a valid directory for .mat processing: {filepath}. Returning empty DataFrame.\")\n",
        "        return DataFrame(columns=COLS)\n",
        "\n",
        "    # Filter for .mat files with the specified monkey prefix\n",
        "    mat_files = [\n",
        "        f for f in os.listdir(filepath) if f.endswith(\".mat\") and f.startswith(monkey_prefix)\n",
        "    ]\n",
        "\n",
        "    if not mat_files:\n",
        "        error(f\"No .mat file found in {filepath} with prefix '{monkey_prefix}'. Returning empty DataFrame.\")\n",
        "        return DataFrame(columns=COLS)\n",
        "\n",
        "    if len(mat_files) > 1:\n",
        "        warning(f\"Multiple .mat files found with prefix '{monkey_prefix}' in {filepath}. \"\n",
        "                f\"Using the first one: {mat_files[0]}.\")\n",
        "\n",
        "    mat_file_to_load = pjoin(filepath, mat_files[0])\n",
        "\n",
        "    data_mat = None # Initialize to None in case of loading error\n",
        "    try:\n",
        "        data_mat = loadmat(mat_file_to_load)[\"genera\"]\n",
        "    except KeyError:\n",
        "        error(f\"Key 'genera' not found in .mat file: {mat_file_to_load}. Check file structure. Returning empty DataFrame.\")\n",
        "        return DataFrame(columns=COLS)\n",
        "    except Exception as e:\n",
        "        error(f\"Error loading .mat file {mat_file_to_load}: {e}. Returning empty DataFrame.\")\n",
        "        return DataFrame(columns=COLS)\n",
        "\n",
        "    if data_mat is None: # Should not happen with current error handling, but good for safety\n",
        "        return DataFrame(columns=COLS)\n",
        "\n",
        "    # Determine start index, handling potential empty header row\n",
        "    # Added more robust check for data_mat shape before accessing indices\n",
        "    if data_mat.shape[0] == 0 or data_mat.shape[1] < 1 or len(data_mat[0, 0]) == 0:\n",
        "         warning(f\"Unexpected .mat 'genera' structure or empty header in {mat_file_to_load}. Assuming idx = 0.\")\n",
        "         idx = 0 # Default to 0 if structure is odd, or if data_mat[0,0] doesn't exist\n",
        "    else:\n",
        "        idx = 1 if (isinstance(data_mat[0, 0], np.ndarray) and data_mat[0, 0].size == 0) else 0\n",
        "\n",
        "    # Extract relevant data parts and stack them. Use checks for shape/content.\n",
        "    try:\n",
        "        # Ensure raw data parts are not empty before squeezing/stacking\n",
        "        if data_mat[idx:, 0].size == 0 or data_mat[idx:, 2].size == 0:\n",
        "            info(f\"No psychophysical data found in {mat_file_to_load} after header processing. Returning empty DataFrame.\")\n",
        "            return DataFrame(columns=COLS)\n",
        "\n",
        "        psico1_raw_stacked = np.stack(data_mat[idx:, 0])\n",
        "        psico2_raw_stacked = np.stack(data_mat[idx:, 2])\n",
        "\n",
        "        psico1 = np.squeeze(psico1_raw_stacked)[:, :20]\n",
        "        psico2 = np.squeeze(psico2_raw_stacked)\n",
        "\n",
        "        # Ensure consistent dimensions for concatenation: convert scalar to 1D array if needed\n",
        "        # This handles cases where squeeze might result in 0-D arrays.\n",
        "        if psico1.ndim == 0: # If scalar, make it 1D\n",
        "            psico1 = np.array([psico1])\n",
        "        elif psico1.ndim == 1:\n",
        "            psico1 = psico1.reshape(-1, 1) # Ensure it's a column vector for consistent concatenation\n",
        "\n",
        "        if psico2.ndim == 0: # If scalar, make it 1D\n",
        "            psico2 = np.array([psico2])\n",
        "        elif psico2.ndim == 1:\n",
        "            psico2 = psico2.reshape(-1, 1) # Ensure it's a column vector for consistent concatenation\n",
        "\n",
        "        # Check if arrays are compatible for concatenation (same number of rows)\n",
        "        if psico1.shape[0] != psico2.shape[0]:\n",
        "            error(f\"Dimension mismatch in psychophysical data from {mat_file_to_load}. Cannot concatenate. Returning empty DataFrame.\")\n",
        "            return DataFrame(columns=COLS)\n",
        "\n",
        "        psico = np.concatenate((psico1, psico2), axis=1)\n",
        "\n",
        "        # Ensure COLS has enough elements for the actual number of columns in 'psico'\n",
        "        if psico.shape[1] > len(COLS):\n",
        "            warning(f\"Psychophysical data has more columns ({psico.shape[1]}) than defined in COLS ({len(COLS)}). Extra columns will be unnamed.\")\n",
        "            # Extend COLS with generic names or handle as needed\n",
        "            extended_cols = COLS + [f\"Unnamed_{i}\" for i in range(psico.shape[1] - len(COLS))]\n",
        "            psico_df = DataFrame(psico, columns=extended_cols)\n",
        "        else:\n",
        "            psico_df = DataFrame(psico, columns=COLS[:psico.shape[1]])\n",
        "\n",
        "    except Exception as e:\n",
        "        error(f\"Error processing psychophysical data from {mat_file_to_load}: {e}. Returning empty DataFrame.\")\n",
        "        return DataFrame(columns=COLS)\n",
        "\n",
        "    # Drop duplicate rows based on 'Trial' column\n",
        "    initial_rows = len(psico_df)\n",
        "    if \"Trial\" in psico_df.columns: # Check if 'Trial' column actually exists\n",
        "        psico_df.drop_duplicates(\"Trial\", inplace=True)\n",
        "        if len(psico_df) < initial_rows:\n",
        "            info(f\"Removed {initial_rows - len(psico_df)} duplicate trial entries from psychometrics.\")\n",
        "    else:\n",
        "        warning(f\"Column 'Trial' not found in psychometric data. Cannot drop duplicates based on trial number.\")\n",
        "\n",
        "\n",
        "    if savepath is not None:\n",
        "        try:\n",
        "            os.makedirs(savepath, exist_ok=True)\n",
        "            info(f\"Ensured save directory exists: {savepath}\")\n",
        "            psico_df.to_csv(pjoin(savepath, \"psychometrics.csv\"), index=False)\n",
        "            info(f\"Saved psychometrics.csv to {savepath}\")\n",
        "        except OSError as e:\n",
        "            error(f\"Failed to create save directory {savepath} or save psychometrics.csv: {e}. Skipping CSV saving.\")\n",
        "        except Exception as e_save:\n",
        "            error(f\"Error saving psychometrics.csv to {savepath}: {e_save}\")\n",
        "\n",
        "    return psico_df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Configuration Parameters ---\n",
        "    # Directory where .nev and .mat raw data files are located\n",
        "    file_path = \"/media/ferxxo/Expansion\"\n",
        "\n",
        "    # Directory where processed data will be saved. The script will create a new\n",
        "    # folder named monkey_name_processed within this path.\n",
        "    save_path = \"/home/ferxxo/Desktop\"\n",
        "\n",
        "    # Name of the monkey, used for folder naming and log file.\n",
        "    monkey_name = \"RR34\"\n",
        "\n",
        "    # Prefix for identifying the .mat file relevant to this monkey (e.g., \"RR034\").\n",
        "    # This makes the mat_processer slightly more flexible.\n",
        "    monkey_mat_prefix = \"RR034\"\n",
        "\n",
        "    # --- Logging Setup ---\n",
        "    # Configure logging to capture and print any errors/info to a .txt file.\n",
        "    log_file_path = pjoin(save_path, f\"py_logging_{monkey_name}.txt\")\n",
        "\n",
        "    # Ensure the directory for the log file exists before configuring logging\n",
        "    log_dir = os.path.dirname(log_file_path)\n",
        "    if log_dir and not os.path.exists(log_dir):\n",
        "        try:\n",
        "            os.makedirs(log_dir, exist_ok=True)\n",
        "        except OSError as e:\n",
        "            # If log directory can't be created, log to console as fallback\n",
        "            print(f\"ERROR: Could not create log directory {log_dir}: {e}. Logging to console.\")\n",
        "            logging.basicConfig(\n",
        "                format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
        "                level=logging.INFO,\n",
        "                datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "            )\n",
        "        else:\n",
        "            logging.basicConfig(\n",
        "                filename=log_file_path,\n",
        "                format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
        "                level=logging.INFO,\n",
        "                datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "            )\n",
        "    else: # If log_dir is empty (current directory) or already exists\n",
        "        logging.basicConfig(\n",
        "            filename=log_file_path,\n",
        "            format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
        "            level=logging.INFO,\n",
        "            datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "        )\n",
        "\n",
        "    # logging.captureWarnings(True) # Uncomment if you want to capture Python warnings\n",
        "\n",
        "    info(f\"Starting data processing for monkey: {monkey_name}\")\n",
        "    info(f\"Source data path: {file_path}\")\n",
        "    info(f\"Processed data save path: {save_path}\")\n",
        "\n",
        "    # --- Execute the Data Processing Pipeline ---\n",
        "    try:\n",
        "        # The spike_pipeline function integrates mat_processer and nev_processer.\n",
        "        # It handles the overall flow and likely merges the outputs for TopoSort.\n",
        "        # Ensure `spike_pipeline` can accept `monkey_mat_prefix` as a keyword argument\n",
        "        # or adapt its signature if necessary.\n",
        "        spike_pipeline(\n",
        "            file_path,\n",
        "            save_path,\n",
        "            monkey_name,\n",
        "            mat_processer,\n",
        "            nev_processer,\n",
        "            monkey_mat_prefix=monkey_mat_prefix\n",
        "        )\n",
        "        info(f\"Data processing for {monkey_name} completed successfully.\")\n",
        "    except Exception as pipeline_e:\n",
        "        error(f\"An unhandled error occurred during the spike_pipeline execution: {pipeline_e}\", exc_info=True)\n",
        "    finally:\n",
        "        # Ensure all pending log messages are written before program exits\n",
        "        logging.shutdown()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "7AxRObJoIDm7",
        "outputId": "d0d7176c-f46d-4882-ec46-9652473119f8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'spike_pipeline'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-784077537.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#from toposort.pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspike_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrailing_number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spike_pipeline'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}